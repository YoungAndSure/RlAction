{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播的对象应该是$J(\\theta)$，但$J(\\theta)$作为累积折扣奖励无法写成$\\theta$相关的函数形式。  \n",
    "所以在$\\nabla_{\\theta}J(\\theta)$求积分得到代理目标，对代理目标求梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下，重读mfrl策略梯度法部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 平均状态价值重推导\n",
    "状态价值定义：  \n",
    "$v_{\\pi}(s)=\\mathbb{E}[G_t|S_t=s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$G_t = R_t + \\gamma G_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= R_t + \\gamma (R_{t+1}+\\gamma G_{t+2}) = R_t + \\gamma R_{t+1} + \\gamma^2 G_{t+2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= \\sum_{t=0}^\\infty \\gamma^t R_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以：  \n",
    "$v_{\\pi}(s) = \\mathbb{E}[\\sum_{t=0}^{\\infty} \\gamma^t R_t|S_t=s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态价值针对的是某个状态，如果对所有状态的改路进行加权，就得到了策略梯度法的目标，平均状态价值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sum_{s\\in\\mathcal{S}} v_{\\pi}(s) = \\sum_{s\\in\\mathcal{S}} d(s)\\mathbb{E}[\\sum_{t=0}^{\\infty}\\gamma^t R_t | S_t =s]=\\bar{v}_{\\pi}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 为什么状态分布$d(s)$可以选择和策略无关的分布，然后还可以作为目标来提升"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "策略是$\\theta$的函数，目标只要是$\\theta$的函数，就可以通过梯度上升来调整$\\theta$提升。  \n",
    "平均状态价值中，两部分，其中$v_{\\pi}(s)$已经是$\\theta$的函数了。因为策略决定了状态分布，也就决定了获取的奖励及状态价值。所以状态价值是$\\theta$的函数。  \n",
    "至于稳定状态分布$d$，理论上说，它和$\\theta$有关，但不是一个可以通过$\\theta$估计出来的值，而是一个长期迭代后收敛的值。所以，本身它也没法作为提升的一部分写进目标公式里，必须要通过蒙特卡洛法来模拟这个分布。  \n",
    "另一方面，调整分布可以设定出不同的目标，让$\\theta$朝着不同的方向更新。比如如果d是所有状态平均，可能这个要解决的问题本身就隐含了希望所有状态的分布是均匀的、同权的，设置目标为平均状态价值，就会让$\\theta$朝着这个方向前进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### average state value 和average reward 有什么区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "average state value:  \n",
    "$\\bar{v}_{\\pi}(s) = \\sum_{s\\in\\mathcal{S}}d_{\\pi}(s)v_{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "average reward:  \n",
    "$\\bar{r}_{\\pi}(s) = \\sum_{s\\in\\mathcal{S}} d_{\\pi}(s)r_{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "猛一看，以为这俩好像没什么区别。好像就是换了个字母。实际上仔细想下状态价值和$r_{\\pi}(s)$含义，就能发现区别。  \n",
    "简单说，状态价值包含即时奖励和未来折扣奖励。也就是说，它的信息里包含了未来的奖励信息。  \n",
    "而$r_{\\pi}(s)$中仅有即时奖励，没有未来的奖励。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v_{\\pi}(s) = \\mathbb{E}[R_t + \\gamma G_{t+1}|S_t=s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$r_{\\pi}(s) = \\mathbb{E}[R_t | S_t=s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个$r_{\\pi}(s)$之前没出现过，所以突然出现有点迷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "换个角度：  \n",
    "$v_{\\pi}(s) = \\sum_{a\\in\\mathcal{A}}\\pi(a|s,\\theta)r(s,a) + \\gamma\\sum_{a\\in\\mathcal{A}}\\pi(a|s,\\theta)\\sum_{s\\in\\mathcal{S}}p(s'|s,a)v_{\\pi}(s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$r_{\\pi}(s) = \\sum_{a\\in\\mathcal{A}}\\pi(a|s,\\theta)r(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也能看出来，$v_{\\pi}(s)$包含了未来折扣奖励，$r_{\\pi}(s)$只有即时奖励。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 怎么理解Box9.1证明的公式9.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lim_{n\\to\\infty}\\frac{1}{n}\\mathbb{E}[\\sum_{t=0}^{n-1}R_{t+1}]=\\sum_{s\\in\\mathcal{S}}d_{\\pi}(s)r_{\\pi}(s)=\\bar{r}_{\\pi}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看完证明回过头来说，公式讲的是，目标$\\bar{r}_{\\pi}$是累积奖励期望的均值。衡量的是，综合整个回合看，执行一个动作可以获得平均奖励。  \n",
    "落到实际蒙特卡洛法任务上，一个任务运行了n步，收集奖励，加和取$\\frac{1}{n}$的平均。运行m次，可以通过平均逼近期望。  \n",
    "拿这个当目标，实际更关注每一步获得的奖励，希望每一步都能得到综合来看最高的即时奖励。而用$\\bar{v}_{\\pi}(s)$做目标，在意的是回合结束后得到的总奖励。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PolicyNet(nn.Module) :\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim) :\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim, dtype=torch.float64)\n",
    "        self.fc2 = nn.Linear(hidden_dim, action_dim, dtype=torch.float64)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        l1 = self.fc1(x)\n",
    "        a1 = F.relu(l1)\n",
    "        l2 = self.fc2(l1)\n",
    "        a2 = F.softmax(l2, dim=1)\n",
    "        #print('x:',x.shape, \"l1:\", l1.shape, \"a1:\", a1.shape, \"l2:\", l2.shape, \"a2:\", a2.shape)\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Agent() :\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, action_space) :\n",
    "        self.policy_net = PolicyNet(state_dim, hidden_dim, action_dim)\n",
    "        self.action_space = action_space\n",
    "        \n",
    "    def take_action(self, state) :\n",
    "        state = torch.tensor(state, dtype=torch.float64).unsqueeze(0)\n",
    "        actions = self.policy_net(state).squeeze()\n",
    "        #print(actions)\n",
    "        action = self.action_space.sample(probability=actions.detach().numpy())\n",
    "        return action\n",
    "\n",
    "    def update(self, observation, reward, done) :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished! Total reward: 11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['SDL_AUDIODRIVER'] = 'dummy'\n",
    "os.environ['XDG_RUNTIME_DIR'] = '/home/youngsure/Code/tmp/'\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "# env init\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "start_observation, info = env.reset(seed=0)\n",
    "#print(observation)\n",
    "\n",
    "# agent init\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "hidden_dim = 128\n",
    "agent = Agent(state_dim, hidden_dim, action_dim, env.action_space)\n",
    "\n",
    "episode_over = False\n",
    "total_reward = 0\n",
    "observation = start_observation\n",
    "\n",
    "while not episode_over:\n",
    "    action = agent.take_action(observation)\n",
    "    next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "    agent.update(observation, reward, terminated or truncated)\n",
    "    \n",
    "    total_reward += reward\n",
    "\n",
    "    observation = next_observation\n",
    "    episode_over = terminated or truncated\n",
    "\n",
    "print(f\"Episode finished! Total reward: {total_reward}\")\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "第9章-策略梯度算法.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ddx",
   "language": "python",
   "name": "ddx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
