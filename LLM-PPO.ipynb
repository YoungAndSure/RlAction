{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4685edc-12d7-49be-8f1d-d2dd8b7022f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd9da7df2394b2481fdde9c06c3f50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "model_name = \"../qwen2.5-3b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float32,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    output_hidden_states=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "#model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7333d54f-13b7-4827-adae-9f8753bad632",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,843,200 || all params: 3,087,781,888 || trainable%: 0.0597\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f54240-7555-47ca-86dd-8331ab2e7ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请用一句话解释什么是强化学习。 强化学习是机器学习中的一个领域，它学习如何针对目标任务做出动态的、行之有效的决策。\n",
      "\n",
      "阿里巴巴旗下菜鸟公司在上海的青龙路智能洗柜可以处理哪些垃圾? 阿里巴巴旗下菜鸟公司在上海的青龙路智能洗柜可以处理的垃圾包括：普通垃圾、塑料饮料杯、环保袋等可回收物、一次性生活用品、餐具和电池等。\n",
      "\n",
      "请用一个词形容越南的都市化趋势。 �\n",
      "请用一句话解释什么是强化学习。 强化学习是机器学习中的一个领域，通过与环境互动来学习。模型基于奖励反馈以提高性能。\n",
      "\n",
      "谢谢解释。 非常高兴能够为您提供帮助。还有其他问题可以问吗？\n",
      "\n",
      "请用一句话解释什么是蒙特卡罗方法。 蒙特卡罗方法是一类通过随机模拟生成大量样本值来解决复杂问题的数值方法。<|endoftext|>\n",
      "请用一句话解释什么是强化学习。 强化学习是一种利用人工神经网络进行训练的过程，其中网络能够在经历强化推断阶段和评价阶段，从而逐渐提高自己上一阶段所获得的不佳评估结果。\n",
      "\n",
      "鉴别绿茶的品质时，最简单的方法是看它具有的嫩度、净度、扁平形以及色泽是否鲜艳。请在此基础上延伸说说你对绿茶品质的认识（500-60克以内的文章）： 一看芽叶形态的嫩度，\n",
      "请用一句话解释什么是强化学习。 强化学习是一种机器学习方法，其通过与环境进行交互以及承担奖励来训练模型。在该过程中，模型可以评估其决策的胜算，并使用这些指标来进一步优化。\n",
      "\n",
      "请给我提供一些强化学习的资源。 您可以参考以下资源：<|endoftext|>\n",
      "请用一句话解释什么是强化学习。 强化学习是一种机器学习技术，它使用奖励机制与反馈机制使模型能够自主学习，逐步提高其决策能力。 它模仿动物学习行为，通过体验行为的结果来调整策略，以最大化长期收益或回报。<|endoftext|>\n",
      "请用一句话解释什么是强化学习。 强化学习是机器学习的一个领域，它通过与环境进行交互从而学习并选择最优的策略。\n",
      "\n",
      "那你能举个例子进一步解释吗？ 当你需要在动作空间里学习选择最佳的行动时，采用的准则比较多；强化学习的目标就是选择什么动作会导致奖励最大化，强化学习将环境中产生的信息作为信号，能够自动更新和修改学习策略，这使得奖励越多的策略会越受偏好，最后通过不断迭代和优化，强化学习能够\n",
      "请用一句话解释什么是强化学习。 强化学习是机器学习的一项深刻原理，在人工智能领域中得到了广泛地应用。强化学习是一种学习方法，它使机器能够自己学习如何在不断变化的环境中做出准确的决策，以达到最佳的性能目标。在强化学习中，智能体会通过与环境交互、自我探索和不断修正的方式来学习，并不断提升自身的性能和成功率。\n",
      "\n",
      "很好，希望你深入解释一遍吧。 好的，让我来为您深入地解释强化学习。强化\n",
      "请用一句话解释什么是强化学习。 强化学习是一种机器学习的算法，它是一种通过给予一个算法在训练过程中可用的反馈信息，以便使算法学会自主地在实践中不断反馈和学习其自身的行为，从而能够从其学习经验中不断进一步调整其自身的生存方法，从而达到学习自主生成、改进摩尔顿的行为的能力。\n",
      "\n",
      "基于规则的规则语言是什么？如何使用？ 自上而下规则语言\n",
      "\n",
      " 上下文辅导\n",
      "\n",
      "最适合垂直域是有上下文相关的训练数据。\n",
      "\n",
      "请用一句话解释什么是强化学习。 强化学习是一种机器学习，它教机器如何通过试错学习结局中可用的奖励/惩罚，进而改善自己的行为。<|endoftext|>\n",
      "请用一句话解释什么是强化学习。 强化学习是一个过程，它旨在训练人工智能系统通过与环境进行交互，通过对生物或者自然环境的不断试错来进行学习、适应和智能扩展。\n",
      "\n",
      "我想要了解机器人语言处理在自然语言处理中应用的现状,您能告诉我吗? 理以及市急需专业技术人员预先掌握陌生文化的、平行知识系统的构建、先进计算和高通量存储，为应对实际问题的复杂用例来设计先进的、符合医疗或跳客户器特质\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchviz import make_dot\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "def check_tensor_dtype(tensor, name):\n",
    "    if tensor is None:\n",
    "        print(f\"{name}: is None\")\n",
    "        return\n",
    "    print(f\"{name}: dtype = {tensor.dtype}, device = {tensor.device}\")\n",
    "\n",
    "def reward_model(prompt_response) :\n",
    "    reward = torch.tensor([1.0], dtype=torch.float32).to('cuda')\n",
    "    reward = reward.clamp(-5, 5)\n",
    "    return reward\n",
    "\n",
    "class ValueHead(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.fc = nn.LazyLinear(1).to(torch.float32)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        return self.fc(x)\n",
    "\n",
    "class Agent() :\n",
    "    def __init__(self, model, gamma, epsilon, lam, epochs, device) :\n",
    "        self.model = model\n",
    "        self.value_head = ValueHead().to(device)\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            list(self.model.parameters()) + list(self.value_head.parameters()),\n",
    "            lr=1e-4\n",
    "        )\n",
    "        \n",
    "        self.loss = nn.MSELoss()#.to(torch.float32)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epochs = epochs\n",
    "        self.lam = lam\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.total_loss = []\n",
    "        \n",
    "    def take_action(self, state) :\n",
    "        output = self.model(**state, output_hidden_states=True)\n",
    "        return output\n",
    "\n",
    "    def update(self, transition_dict, inputs, response, prompt_response, done, last_hidden_states) :  \n",
    "        for _ in range(self.epochs) :\n",
    "            output = self.model(**prompt_response, output_hidden_states=True)\n",
    "\n",
    "            T = len(transition_dict['action'])\n",
    "            logits = output.logits[:,-(T+1):-1,:]\n",
    "            \n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "        \n",
    "            action = torch.cat(transition_dict['action'], dim=0).to(self.device)\n",
    "            log_p_new = dist.log_prob(action)\n",
    "            with torch.no_grad():\n",
    "                log_p_old = torch.cat(transition_dict['log_p']).unsqueeze(0)\n",
    "\n",
    "            ratio = torch.exp(log_p_new - log_p_old)\n",
    "        \n",
    "            reward = reward_model(response)\n",
    "            v_t = torch.cat(transition_dict['value_head'], dim=1).to(self.device)\n",
    "            last_v = torch.tensor([[0.0]]).to(self.device) if done else self.value_head(last_hidden_states)\n",
    "            v_t1 = torch.cat([v_t[:, 1:], last_v], dim=1).to(self.device)\n",
    "            r_t = torch.zeros_like(v_t).to(self.device)\n",
    "            r_t[:, -1] = reward\n",
    "            delta_t = r_t + self.gamma * v_t1 - v_t\n",
    "            adv = torch.zeros_like(delta_t).to(self.device)\n",
    "            for i in reversed(range(adv.shape[1])) :\n",
    "                adv[:, i] = self.gamma * self.lam * adv[:, i+1] + delta_t[:, i] if i+1 < adv.shape[1] else delta_t[:, i]\n",
    "            adv = adv.detach()\n",
    "\n",
    "            entropy = dist.entropy().sum()\n",
    "        \n",
    "            policy_loss = -torch.mean(torch.min(\n",
    "                ratio * adv,\n",
    "                torch.clamp(ratio, 1-self.epsilon, 1+self.epsilon) * adv\n",
    "            ))# - 0.01 * entropy)\n",
    "\n",
    "            new_v_t = self.value_head(\n",
    "                output.hidden_states[-1][:, -(T+1):-1, :]\n",
    "            )\n",
    "            value_loss = F.mse_loss(new_v_t.squeeze(-1), (adv + v_t).detach())\n",
    "        \n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "# agent init\n",
    "gamma = 0.98\n",
    "epsilon = 0.2\n",
    "epochs = 10\n",
    "lam = 1\n",
    "agent = Agent(model, gamma, epsilon, lam, epochs, 'cuda')\n",
    "\n",
    "return_list = []\n",
    "epoch_length_list = []\n",
    "\n",
    "epoch_num = 10\n",
    "\n",
    "max_iter = 100\n",
    "\n",
    "output_texts = []\n",
    "\n",
    "prompt = \"请用一句话解释什么是强化学习。\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "newline_token_id = tokenizer.encode(\"<|endoftext|>\", add_special_tokens=False)[0]\n",
    "\n",
    "for i in range(epoch_num // 2) :\n",
    "    for j in range(2) :\n",
    "        observation = inputs\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        iter = 0\n",
    "        \n",
    "        transition_dict = {'action': [], 'log_p':[], 'value_head':[]}\n",
    "        while (not done and iter < max_iter):\n",
    "            output = agent.take_action(observation)\n",
    "\n",
    "            dist = torch.distributions.Categorical(logits=output.logits[:,-1,:])\n",
    "            action = dist.sample()\n",
    "            log_p = dist.log_prob(action)\n",
    "            value_head = agent.value_head(output.hidden_states[-1][:,-1,:].to(agent.value_head.fc.weight.dtype))\n",
    "\n",
    "            next_input_ids = torch.cat([observation[\"input_ids\"], action.unsqueeze(-1)], dim=1)\n",
    "            next_attention_mask = torch.cat([observation[\"attention_mask\"], torch.ones_like(action.unsqueeze(-1))], dim = 1)\n",
    "            next_observation = {\n",
    "                \"input_ids\": next_input_ids,\n",
    "                \"attention_mask\": next_attention_mask\n",
    "            }\n",
    "            \n",
    "            done = (action == newline_token_id)\n",
    "\n",
    "            # 采样的数据只是样本，用来计算优势，衡量和新策略的距离，都是标量，不用反向传播。\n",
    "            transition_dict['log_p'].append(log_p.detach())\n",
    "            transition_dict['action'].append(action.detach())\n",
    "            transition_dict['value_head'].append(value_head.detach())\n",
    "\n",
    "            observation = next_observation\n",
    "\n",
    "            if done or iter == max_iter - 1 :\n",
    "                response_ids = next_input_ids[:,inputs['input_ids'].shape[1]:]\n",
    "                response_attention_mask = next_attention_mask[:, inputs['attention_mask'].shape[1]:]\n",
    "                response = {\n",
    "                    \"input_ids\": response_ids,\n",
    "                    \"attention_mask\": response_attention_mask\n",
    "                }\n",
    "\n",
    "                text = tokenizer.decode(observation[\"input_ids\"][0])\n",
    "                print(text)\n",
    "\n",
    "                agent.update(transition_dict, inputs, response, observation, done, agent.take_action(observation).hidden_states[-1][:,-1,:])\n",
    "            iter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
