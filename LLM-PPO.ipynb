{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4685edc-12d7-49be-8f1d-d2dd8b7022f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f86497c5ac6436296af7efc85aac449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "model_name = \"../qwen2.5-3b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    output_hidden_states=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "#model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7333d54f-13b7-4827-adae-9f8753bad632",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,843,200 || all params: 3,087,781,888 || trainable%: 0.0597\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f54f34e9-87db-4ad1-800a-3edbc019d05e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 14880,  11622, 105321, 104136, 106582, 103980, 100134,   1773]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "inputs: {'input_ids': tensor([[ 14880,  11622, 105321, 104136, 106582, 103980, 100134,   1773]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "output: torch.Size([1, 8, 151936])\n",
      "input_text: 请用一句话解释什么是强化学习。\n",
      "output_text: 地中文描述什么是人工智能学习。\n",
      " �\n",
      "target token prob before update: 5.960464477539063e-08\n",
      "inputs: {'input_ids': tensor([[ 14880,  11622, 105321, 104136, 106582, 103980, 100134,   1773]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "output: torch.Size([1, 8, 151936])\n",
      "input_text: 请用一句话解释什么是强化学习。\n",
      "output_text: 地中文描述什么是人工智能学习。\n",
      " �\n",
      "target token prob before update: 5.960464477539063e-08\n",
      "target token prob after update: 1.1920928955078125e-07\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "prompt = \"请用一句话解释什么是强化学习。\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "print(inputs)\n",
    "\n",
    "for i in range(2) :\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits  # [B, T, V]\n",
    "    print(\"inputs:\", inputs)\n",
    "    print(\"output:\", outputs.logits.shape)\n",
    "    \n",
    "    print(\"input_text:\", tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True))\n",
    "    text = tokenizer.decode(outputs.logits.argmax(dim=-1)[0])\n",
    "    print(\"output_text:\", text)\n",
    "\n",
    "    # 取最后一个 token 的 logits\n",
    "    last_logits = logits[:, -1, :]\n",
    "\n",
    "    # 假设我们想“鼓励生成 token id = 42”\n",
    "    target_token_id = torch.tensor([42], device=logits.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        old_prob = torch.softmax(last_logits, dim=-1)[0, target_token_id]\n",
    "        print(\"target token prob before update:\", old_prob.item())\n",
    "\n",
    "    loss = -torch.log_softmax(last_logits, dim=-1)[0, target_token_id]\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=1e-4\n",
    "    )\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_new = model(**inputs)\n",
    "    new_logits = outputs_new.logits[:, -1, :]\n",
    "    new_prob = torch.softmax(new_logits, dim=-1)[0, target_token_id]\n",
    "print(\"target token prob after update:\", new_prob.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0f54240-7555-47ca-86dd-8331ab2e7ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_371324/4205823548.py:69: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1857.)\n",
      "  adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/TensorCompare.cu:112: _assert_async_cuda_kernel: block: [0,0,0], thread: [0,0,0] Assertion `probability tensor contains either `inf`, `nan` or element < 0` failed.\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 128\u001b[39m\n\u001b[32m    122\u001b[39m next_attention_mask = torch.cat([observation[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m], torch.ones_like(action.unsqueeze(-\u001b[32m1\u001b[39m))], dim = \u001b[32m1\u001b[39m)\n\u001b[32m    123\u001b[39m next_observation = {\n\u001b[32m    124\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: next_input_ids,\n\u001b[32m    125\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m: next_attention_mask\n\u001b[32m    126\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m text = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_input_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m#print(\"output_text:\", text)\u001b[39;00m\n\u001b[32m    130\u001b[39m \n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m#print(action, observation[\"input_ids\"], observation[\"attention_mask\"], next_input_ids, next_attention_mask)\u001b[39;00m\n\u001b[32m    133\u001b[39m done = (action == newline_token_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ddx/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:4057\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m   4036\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4037\u001b[39m \u001b[33;03mConverts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[32m   4038\u001b[39m \u001b[33;03mtokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4054\u001b[39m \u001b[33;03m    `str`: The decoded sentence.\u001b[39;00m\n\u001b[32m   4055\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4056\u001b[39m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4057\u001b[39m token_ids = \u001b[43mto_py_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4059\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decode(\n\u001b[32m   4060\u001b[39m     token_ids=token_ids,\n\u001b[32m   4061\u001b[39m     skip_special_tokens=skip_special_tokens,\n\u001b[32m   4062\u001b[39m     clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n\u001b[32m   4063\u001b[39m     **kwargs,\n\u001b[32m   4064\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ddx/lib/python3.13/site-packages/transformers/utils/generic.py:273\u001b[39m, in \u001b[36mto_py_obj\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m framework, test_func \u001b[38;5;129;01min\u001b[39;00m framework_to_test_func.items():\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m test_func(obj):\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mframework_to_py_obj\u001b[49m\u001b[43m[\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[38;5;66;03m# tolist also works on 0d np arrays\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, np.number):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ddx/lib/python3.13/site-packages/transformers/utils/generic.py:263\u001b[39m, in \u001b[36mto_py_obj.<locals>.<lambda>\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m    259\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [to_py_obj(o) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[32m    262\u001b[39m framework_to_py_obj = {\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    264\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: obj.numpy().tolist(),\n\u001b[32m    265\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mjax\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: np.asarray(obj).tolist(),\n\u001b[32m    266\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnp\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: obj.tolist(),\n\u001b[32m    267\u001b[39m }\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# This gives us a smart order to test the frameworks with the corresponding tests.\u001b[39;00m\n\u001b[32m    270\u001b[39m framework_to_test_func = _get_frameworks_and_test_func(obj)\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchviz import make_dot\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def check_tensor_dtype(tensor, name):\n",
    "    if tensor is None:\n",
    "        print(f\"{name}: is None\")\n",
    "        return\n",
    "    print(f\"{name}: dtype = {tensor.dtype}, device = {tensor.device}\")\n",
    "\n",
    "def reward_model(prompt_response) :\n",
    "    reward = torch.tensor([1.0], dtype=torch.float16).to('cuda')\n",
    "    reward = reward.clamp(-5, 5)\n",
    "    return reward\n",
    "\n",
    "class ValueHead(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.fc = nn.LazyLinear(1).to(torch.float16)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        return self.fc(x)\n",
    "\n",
    "class Agent() :\n",
    "    def __init__(self, model, gamma, epsilon, lam, epochs, device) :\n",
    "        self.model = model\n",
    "        self.value_head = ValueHead().to(device)\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            list(self.model.parameters()) + list(self.value_head.parameters()),\n",
    "            lr=1e-4\n",
    "        )\n",
    "        \n",
    "        self.loss = nn.MSELoss().to(torch.float16)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epochs = epochs\n",
    "        self.lam = lam\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.total_loss = []\n",
    "        \n",
    "    def take_action(self, state) :\n",
    "        output = self.model(**state, output_hidden_states=True)\n",
    "        return output\n",
    "\n",
    "    def update(self, transition_dict, inputs, response, prompt_response) :  \n",
    "        for _ in range(self.epochs) :\n",
    "            output = self.model(**prompt_response, output_hidden_states=True)\n",
    "        \n",
    "            T = len(transition_dict['action'])\n",
    "            logits = output.logits[:,-T:,:]\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "        \n",
    "            action = torch.cat(transition_dict['action'], dim=0).to(self.device)\n",
    "            log_p_new = dist.log_prob(action).sum()\n",
    "            with torch.no_grad():\n",
    "                log_p_old = torch.stack(transition_dict['log_p']).sum()\n",
    "        \n",
    "            ratio = torch.exp(log_p_new - log_p_old)\n",
    "        \n",
    "            reward = reward_model(response)\n",
    "            adv = (reward - transition_dict['value_head'][-1]).detach()\n",
    "            adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "            entropy = dist.entropy().sum()\n",
    "        \n",
    "            policy_loss = -torch.min(\n",
    "                ratio * adv,\n",
    "                torch.clamp(ratio, 1-self.epsilon, 1+self.epsilon) * adv\n",
    "            ) - 0.01 * entropy\n",
    "\n",
    "            new_value = self.value_head(\n",
    "                output.hidden_states[-1][:, -1, :]\n",
    "            )\n",
    "            value_loss = F.mse_loss(new_value.squeeze(), reward.squeeze())\n",
    "        \n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        print(loss.item())\n",
    "\n",
    "# agent init\n",
    "gamma = 0.98\n",
    "epsilon = 0.2\n",
    "epochs = 10\n",
    "lam = 1\n",
    "agent = Agent(model, gamma, epsilon, lam, epochs, 'cuda')\n",
    "\n",
    "return_list = []\n",
    "epoch_length_list = []\n",
    "\n",
    "epoch_num = 10\n",
    "\n",
    "prompt = \"请用一句话解释什么是强化学习。\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "newline_token_id = tokenizer.encode(\"<|endoftext|>\", add_special_tokens=False)[0]\n",
    "\n",
    "for i in range(epoch_num // 2) :\n",
    "    for j in range(2) :\n",
    "        observation = inputs\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        iter = 0\n",
    "        \n",
    "        transition_dict = {'action': [], 'log_p':[], 'value_head':[]}\n",
    "        while (not done and iter < 100):\n",
    "            output = agent.take_action(observation)\n",
    "            dist = torch.distributions.Categorical(logits=output.logits[:,-1,:])\n",
    "            action = dist.sample()\n",
    "            log_p = dist.log_prob(action)\n",
    "            value_head = agent.value_head(output.hidden_states[-1][:,-1,:])\n",
    "\n",
    "            next_input_ids = torch.cat([observation[\"input_ids\"], action.unsqueeze(-1)], dim=1)\n",
    "            next_attention_mask = torch.cat([observation[\"attention_mask\"], torch.ones_like(action.unsqueeze(-1))], dim = 1)\n",
    "            next_observation = {\n",
    "                \"input_ids\": next_input_ids,\n",
    "                \"attention_mask\": next_attention_mask\n",
    "            }\n",
    "            \n",
    "            text = tokenizer.decode(next_input_ids[0])\n",
    "            #print(\"output_text:\", text)\n",
    "            \n",
    "            #print(action, observation[\"input_ids\"], observation[\"attention_mask\"], next_input_ids, next_attention_mask)\n",
    "            \n",
    "            done = (action == newline_token_id)\n",
    "        \n",
    "            # 采样的数据只是样本，用来计算优势，衡量和新策略的距离，都是标量，不用反向传播。\n",
    "            transition_dict['log_p'].append(log_p.detach())\n",
    "            transition_dict['action'].append(action.detach())\n",
    "            transition_dict['value_head'].append(value_head.detach())\n",
    "\n",
    "            if done :\n",
    "                #print(next_observation['input_ids'], torch.cat(transition_dict['action'], dim=0))\n",
    "                response_ids = next_input_ids[:,inputs['input_ids'].shape[1]:]\n",
    "                response_attention_mask = next_attention_mask[:, inputs['attention_mask'].shape[1]:]\n",
    "                response = {\n",
    "                    \"input_ids\": response_ids,\n",
    "                    \"attention_mask\": response_attention_mask\n",
    "                }\n",
    "                agent.update(transition_dict, inputs, response, next_observation)\n",
    "            observation = next_observation\n",
    "            iter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddx",
   "language": "python",
   "name": "ddx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
