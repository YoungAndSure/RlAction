{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "009d8bc8-6044-42cc-8044-ad9319ed04d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rewards = [-1, -2, -2, 10, 1, 0]\n",
    "def compute_return(start_index, chain, gamma) :\n",
    "    G = 0\n",
    "    # 注意，必须从后往前计算\n",
    "    for state in reversed(chain) :\n",
    "        r = rewards[state - 1]\n",
    "        G = r + gamma * G\n",
    "    return G\n",
    "\n",
    "chain = [1, 2, 3, 6]\n",
    "start_index = 0\n",
    "gamma = 0.5\n",
    "G = compute_return(start_index, chain, gamma)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71116f-d403-4148-b2e2-ef68ab5f62b1",
   "metadata": {},
   "source": [
    "矩阵形式贝尔曼方程：  \n",
    "$v = r + \\gamma P v$  \n",
    "解：  \n",
    "$v = (I - \\gamma P)^{-1}r$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8aff8603-e898-44bc-b010-2384c18209dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.01950168]\n",
      " [-2.21451846]\n",
      " [ 1.16142785]\n",
      " [10.53809283]\n",
      " [ 3.58728554]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def compute(gamma, P, rewards, state_num) :\n",
    "    I = np.eye(state_num)\n",
    "    rewards = np.array(rewards).reshape((-1, 1))\n",
    "    state_value = np.dot(np.linalg.inv(I - gamma * P), rewards)\n",
    "    return state_value\n",
    "\n",
    "P = [\n",
    "    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],\n",
    "    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "]\n",
    "P = np.array(P)\n",
    "# 就算不reshape, numpy也会拿行向量当列向量计算。\n",
    "state_value = compute(gamma, P, rewards, 6)\n",
    "print(state_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e74f6fe-efdf-490d-ba58-34bfc492256d",
   "metadata": {},
   "source": [
    "只看公式，不动手写写，总觉得不得劲"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5570d80-8e03-4b83-9ab3-d2c80bf6a652",
   "metadata": {},
   "source": [
    "### Markov Process:  \n",
    "$(\\mathcal{S},\\mathcal{P})$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64223a91-128c-4c2c-bfe8-07f83154156c",
   "metadata": {},
   "source": [
    "### Markov Reward Process:  \n",
    "$(\\mathcal{S},\\mathcal{P},\\mathcal{R},\\gamma)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4885817-65f5-4608-ac05-4d977880b85c",
   "metadata": {},
   "source": [
    "#### t时刻开始的回报:  \n",
    "$G_t=R_t + \\gamma G_{t+1}$  \n",
    "#### 价值函数:  \n",
    "##### 期望形式:  \n",
    "$\n",
    "\\begin{align}\n",
    "V(s)&=\\mathbb{E}[G_t|S_t = s]\\\\\n",
    "&=\\mathbb{E}[R_t+\\gamma G_{t+1}|S_t=s]\\\\\n",
    "&=\\mathbb{E}[R_t|S_t=s] + \\gamma \\mathbb{E}[G_{t+1}|S_t=s]\n",
    "\\end{align}\n",
    "$  \n",
    "$\\mathbb{E}[G_{t+1}|S_t=s]$和$\\mathbb{E}[G_{t+1}|S_t=s,S_{t+1}=s']$还是有区别的。理论上$G_{t+1}$和$s'$有关，因此前者要展开成后者。根据全期望公式，$\\mathbb{E}[X]=\\mathbb{E}[\\mathbb{E}[X|Y]]$  \n",
    "$\\mathbb{E}[G_{t+1}|S_t=s]=\\sum_{s'\\in\\mathcal{S}}p(s'|s)\\mathbb{E}[G_{t+1}|S_t=s,S_{t+1}=s']$  \n",
    "对应全期望公式里，$X=\\{G_{t+1}|S_t=s\\}$,$X|Y=\\{G_{t+1}|S_t=s,S_{t+1}=s'\\}$,$Y=\\{S_{t+1}=s\\}$  \n",
    "由于马尔科夫性，$G_{t+1}$和$S_{t}=s$无关，因此$\\mathbb{E}[G_{t+1}|S_t=s,S_{t+1}=s']=\\mathbb{E}[G_{t+1}|S_{t+1}=s']$  \n",
    "$\\mathbb{E}[G_{t+1}|S_t=s]=\\sum_{s'\\in\\mathcal{S}}p(s'|s) \\mathbb{E}[G_{t+1}|S_{t+1}=s']=\\sum_{s'\\in\\mathcal{S}}p(s'|s) V(s') = \\mathbb{E}[V(s')|S_t=s]$  \n",
    "最终：  \n",
    "$V(s) = \\mathbb{E}[R_t|S_t=s] + \\gamma \\mathbb{E}[V(s')|S_t=s]=\\mathbb{E}[R_t + \\gamma V(s')|S_t=s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b83ba0-247b-4444-9faf-555a0af0cdf0",
   "metadata": {},
   "source": [
    "##### 期望展开形式:\n",
    "$V(s)=R_t(s) + \\gamma \\sum_{s' \\in \\mathcal{S}}p(s'|s)V(s')$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d940eb-e297-4ec0-86b3-b74fb2b6e3b0",
   "metadata": {},
   "source": [
    "#### 矩阵形式:  \n",
    "$V = r+\\gamma PV$  \n",
    "$V = (I-\\gamma P)^{-1} r$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844ac2e-136e-40b8-9742-e12bb706e392",
   "metadata": {},
   "source": [
    "### Markov Decision Process\n",
    "$(\\mathcal{S},\\mathcal{A},\\mathcal{R},\\mathcal{P},\\gamma)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e50b3e-856b-4bb6-aeeb-d615b83f4a80",
   "metadata": {},
   "source": [
    "#### 状态价值\n",
    "$V_{\\pi} = \\mathbb{E}[G_t|S_t=s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef5efe0-e3a2-4e71-8867-b996d1cabdd1",
   "metadata": {},
   "source": [
    "#### 行动价值\n",
    "$Q_{\\pi} = \\mathbb{E}[G_t|S_t=s,A_t=a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89bb62c-c1fc-4fca-b7e8-eaa3177fc1c9",
   "metadata": {},
   "source": [
    "#### 状态价值和行动价值关系\n",
    "$V_{\\pi} = \\sum_{a\\in\\mathcal{A}}\\pi(s,a)Q_{\\pi}(s,a)$  \n",
    "$Q_{\\pi} = r_t(s,a) + \\gamma \\sum_{s' \\in\\mathcal{S}}p(s'|s,a)V_{\\pi}(s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4eb28f-cef1-4480-a4c5-6e9d07ac4642",
   "metadata": {},
   "source": [
    "#### 贝尔曼方程展开\n",
    "$\n",
    "\\begin{align}\n",
    "V_{\\pi}(s) &= \\mathbb{E}[R_t + \\gamma V_{\\pi}(s')|S_t=s]\\\\\n",
    "&= \\sum_{a\\in\\mathcal{A}}\\pi(a|s)[r(s,a)+\\gamma \\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V_{\\pi}(s')]\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d6afbd-9316-4367-82ae-a7fed66696df",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "Q_{\\pi}(s,a) &= r_t(s,a) + \\gamma \\mathbb{E}[V_{\\pi}(s')|S_t=s] \\\\\n",
    "&= r_t(s,a) + \\gamma \\sum_{s'\\in\\mathcal{S}}p(s'|s,a)\\sum_{a'\\in\\mathcal{A}}\\pi(a'|s')Q_{\\pi}(s',a')\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c974fd5a-a34f-4ab4-8bb6-b244747c54fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"]  # 状态集合\n",
    "A = [\"保持s1\", \"前往s1\", \"前往s2\", \"前往s3\", \"前往s4\", \"前往s5\", \"概率前往\"]  # 动作集合\n",
    "# 状态转移函数\n",
    "P = {\n",
    "    \"s1-保持s1-s1\": 1.0,\n",
    "    \"s1-前往s2-s2\": 1.0,\n",
    "    \"s2-前往s1-s1\": 1.0,\n",
    "    \"s2-前往s3-s3\": 1.0,\n",
    "    \"s3-前往s4-s4\": 1.0,\n",
    "    \"s3-前往s5-s5\": 1.0,\n",
    "    \"s4-前往s5-s5\": 1.0,\n",
    "    \"s4-概率前往-s2\": 0.2,\n",
    "    \"s4-概率前往-s3\": 0.4,\n",
    "    \"s4-概率前往-s4\": 0.4,\n",
    "}\n",
    "# 奖励函数\n",
    "R = {\n",
    "    \"s1-保持s1\": -1,\n",
    "    \"s1-前往s2\": 0,\n",
    "    \"s2-前往s1\": -1,\n",
    "    \"s2-前往s3\": -2,\n",
    "    \"s3-前往s4\": -2,\n",
    "    \"s3-前往s5\": 0,\n",
    "    \"s4-前往s5\": 10,\n",
    "    \"s4-概率前往\": 1,\n",
    "}\n",
    "gamma = 0.5  # 折扣因子\n",
    "MDP = (S, A, P, R, gamma)\n",
    "\n",
    "# 策略1,随机策略\n",
    "Pi_1 = {\n",
    "    \"s1-保持s1\": 0.5,\n",
    "    \"s1-前往s2\": 0.5,\n",
    "    \"s2-前往s1\": 0.5,\n",
    "    \"s2-前往s3\": 0.5,\n",
    "    \"s3-前往s4\": 0.5,\n",
    "    \"s3-前往s5\": 0.5,\n",
    "    \"s4-前往s5\": 0.5,\n",
    "    \"s4-概率前往\": 0.5,\n",
    "}\n",
    "# 策略2\n",
    "Pi_2 = {\n",
    "    \"s1-保持s1\": 0.6,\n",
    "    \"s1-前往s2\": 0.4,\n",
    "    \"s2-前往s1\": 0.3,\n",
    "    \"s2-前往s3\": 0.7,\n",
    "    \"s3-前往s4\": 0.5,\n",
    "    \"s3-前往s5\": 0.5,\n",
    "    \"s4-前往s5\": 0.1,\n",
    "    \"s4-概率前往\": 0.9,\n",
    "}\n",
    "\n",
    "\n",
    "# 把输入的两个字符串通过“-”连接,便于使用上述定义的P、R变量\n",
    "def join(str1, str2):\n",
    "    return str1 + '-' + str2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f9b4fbd-4175-459d-a09a-ecd2393c91ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP中每个状态价值分别为\n",
      " [[-1.22555411]\n",
      " [-1.67666232]\n",
      " [ 0.51890482]\n",
      " [ 6.0756193 ]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.5\n",
    "# 转化后的MRP的状态转移矩阵\n",
    "P_from_mdp_to_mrp = [\n",
    "    [0.5, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.5, 0.5],\n",
    "    [0.0, 0.1, 0.2, 0.2, 0.5],\n",
    "    [0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "]\n",
    "P_from_mdp_to_mrp = np.array(P_from_mdp_to_mrp)\n",
    "R_from_mdp_to_mrp = [-0.5, -1.5, -1.0, 5.5, 0]\n",
    "\n",
    "V = compute(gamma, P_from_mdp_to_mrp, R_from_mdp_to_mrp, 5)\n",
    "print(\"MDP中每个状态价值分别为\\n\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74c8f81-e4d3-41f2-983b-16103bc96380",
   "metadata": {},
   "source": [
    "MDP是怎么转成MRP的？  \n",
    "我原以为是策略矩阵和状态转移矩阵直接乘就可以了。但实际看了维度，对不上。  \n",
    "策略$\\pi(a|s)$的shape:(state_num, action_num)\n",
    "状态转移概率$`p(s'|s,a)`$的shape:(state_num, action_num, state_num)\n",
    "对于某个状态，从策略里切出对应的状态，shape是(action_num, 1)，转置成向量，shape(1, action_num)  \n",
    "从状态转移概率里切出对应的状态，shape是(action_num, state_num)  \n",
    "两者相乘，(1, action_num) 乘 (action_num, state_num) 等于(1, state_num)  \n",
    "对于每个状态，都来一遍，所以最终是(state_num, state_num)，也就是状态转移概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78fb01a1-33d6-4596-af2b-3c5ae5d03909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一条序列\n",
      " [('s1', '前往s2', 0, 's2'), ('s2', '前往s1', -1, 's1'), ('s1', '前往s2', 0, 's2'), ('s2', '前往s1', -1, 's1'), ('s1', '保持s1', -1, 's1'), ('s1', '保持s1', -1, 's1'), ('s1', '保持s1', -1, 's1'), ('s1', '保持s1', -1, 's1'), ('s1', '前往s2', 0, 's2'), ('s2', '前往s3', -2, 's3'), ('s3', '前往s4', -2, 's4'), ('s4', '概率前往', 1, 's2'), ('s2', '前往s3', -2, 's3'), ('s3', '前往s5', 0, 's5')]\n",
      "第二条序列\n",
      " [('s2', '前往s1', -1, 's1'), ('s1', '保持s1', -1, 's1'), ('s1', '保持s1', -1, 's1'), ('s1', '前往s2', 0, 's2'), ('s2', '前往s1', -1, 's1'), ('s1', '保持s1', -1, 's1'), ('s1', '保持s1', -1, 's1'), ('s1', '保持s1', -1, 's1'), ('s1', '保持s1', -1, 's1'), ('s1', '保持s1', -1, 's1'), ('s1', '保持s1', -1, 's1'), ('s1', '前往s2', 0, 's2'), ('s2', '前往s1', -1, 's1'), ('s1', '前往s2', 0, 's2'), ('s2', '前往s3', -2, 's3'), ('s3', '前往s4', -2, 's4'), ('s4', '概率前往', 1, 's4'), ('s4', '概率前往', 1, 's3'), ('s3', '前往s4', -2, 's4'), ('s4', '概率前往', 1, 's3'), ('s3', '前往s4', -2, 's4')]\n",
      "第五条序列\n",
      " [('s2', '前往s3', -2, 's3'), ('s3', '前往s4', -2, 's4'), ('s4', '前往s5', 10, 's5')]\n"
     ]
    }
   ],
   "source": [
    "def sample(MDP, Pi, timestep_max, number):\n",
    "    ''' 采样函数,策略Pi,限制最长时间步timestep_max,总共采样序列数number '''\n",
    "    S, A, P, R, gamma = MDP\n",
    "    episodes = []\n",
    "    for _ in range(number):\n",
    "        episode = []\n",
    "        timestep = 0\n",
    "        s = S[np.random.randint(4)]  # 随机选择一个除s5以外的状态s作为起点\n",
    "        # 当前状态为终止状态或者时间步太长时,一次采样结束\n",
    "        while s != \"s5\" and timestep <= timestep_max:\n",
    "            timestep += 1\n",
    "            rand, temp = np.random.rand(), 0\n",
    "            # 在状态s下根据策略选择动作\n",
    "            for a_opt in A:\n",
    "                temp += Pi.get(join(s, a_opt), 0)\n",
    "                if temp > rand:\n",
    "                    a = a_opt\n",
    "                    r = R.get(join(s, a), 0)\n",
    "                    break\n",
    "            rand, temp = np.random.rand(), 0\n",
    "            # 根据状态转移概率得到下一个状态s_next\n",
    "            for s_opt in S:\n",
    "                temp += P.get(join(join(s, a), s_opt), 0)\n",
    "                if temp > rand:\n",
    "                    s_next = s_opt\n",
    "                    break\n",
    "            episode.append((s, a, r, s_next))  # 把（s,a,r,s_next）元组放入序列中\n",
    "            s = s_next  # s_next变成当前状态,开始接下来的循环\n",
    "        episodes.append(episode)\n",
    "    return episodes\n",
    "\n",
    "\n",
    "# 采样5次,每个序列最长不超过20步\n",
    "episodes = sample(MDP, Pi_1, 20, 5)\n",
    "print('第一条序列\\n', episodes[0])\n",
    "print('第二条序列\\n', episodes[1])\n",
    "print('第五条序列\\n', episodes[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62d49bbf-76f3-402e-884d-abaa206fbce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用蒙特卡洛方法计算MDP的状态价值为\n",
      " {'s1': -1.234926404480339, 's2': -1.6920408505160804, 's3': 0.4596046559950893, 's4': 5.910741298902233, 's5': 0}\n"
     ]
    }
   ],
   "source": [
    "# 对所有采样序列计算所有状态的价值\n",
    "def MC(episodes, V, N, gamma):\n",
    "    for episode in episodes:\n",
    "        G = 0\n",
    "        for i in range(len(episode) - 1, -1, -1):  #一个序列从后往前计算\n",
    "            (s, a, r, s_next) = episode[i]\n",
    "            G = r + gamma * G\n",
    "            N[s] = N[s] + 1\n",
    "            V[s] = V[s] + (G - V[s]) / N[s]\n",
    "\n",
    "\n",
    "timestep_max = 20\n",
    "# 采样1000次,可以自行修改\n",
    "episodes = sample(MDP, Pi_1, timestep_max, 1000)\n",
    "gamma = 0.5\n",
    "V = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\n",
    "N = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\n",
    "MC(episodes, V, N, gamma)\n",
    "print(\"使用蒙特卡洛方法计算MDP的状态价值为\\n\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fbd87170-a056-4d19-a03a-745d7265528c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11686852128509743 0.23391363418625283\n"
     ]
    }
   ],
   "source": [
    "def occupancy(episodes, s, a, timestep_max, gamma):\n",
    "    ''' 计算状态动作对（s,a）出现的频率,以此来估算策略的占用度量 '''\n",
    "    '''\n",
    "     episodes里有多条轨迹，每条轨迹长度不同，理论上total_times的长度是最长轨迹的长度，为了省略，直接写1000\n",
    "     所以，这个total_count，第i个元素，就是\"有多少回合走到了这个时间步\",理论上每个回合肯定有第一个时间步，\n",
    "     所以total_times[0] == len(episodes)\n",
    "     occur_times[i]是指定的(s,a)对，在时刻i出现过，就+1.\n",
    "     所以occur_times[i] / total_times[i]的意思就是，有total_times[i]个回合进行到了时间步i，\n",
    "     其中指定(s,a)对在i时刻出现occur_times[i]次.也就是所谓的,(s,a)在某时刻出现的概率(估计)。\n",
    "    '''\n",
    "    rho = 0\n",
    "    total_times = np.zeros(timestep_max)  # 记录每个时间步t各被经历过几次\n",
    "    occur_times = np.zeros(timestep_max)  # 记录(s_t,a_t)=(s,a)的次数\n",
    "    for episode in episodes:\n",
    "        for i in range(len(episode)):\n",
    "            (s_opt, a_opt, r, s_next) = episode[i]\n",
    "            total_times[i] += 1\n",
    "            if s == s_opt and a == a_opt:\n",
    "                occur_times[i] += 1\n",
    "    for i in reversed(range(timestep_max)):\n",
    "        if total_times[i]:\n",
    "            rho += gamma**i * occur_times[i] / total_times[i]\n",
    "    return (1 - gamma) * rho\n",
    "\n",
    "gamma = 0.5\n",
    "timestep_max = 1000\n",
    "\n",
    "np.random.seed(1)\n",
    "episodes_1 = sample(MDP, Pi_1, timestep_max, 1000)\n",
    "episodes_2 = sample(MDP, Pi_2, timestep_max, 1000)\n",
    "rho_1 = occupancy(episodes_1, \"s4\", \"概率前往\", timestep_max, gamma)\n",
    "rho_2 = occupancy(episodes_2, \"s4\", \"概率前往\", timestep_max, gamma)\n",
    "print(rho_1, rho_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb5df5-375a-41b7-9f8d-5098091c30b8",
   "metadata": {},
   "source": [
    "#### 状态访问分布 state visitation distribution\n",
    "吐槽一下，这本书写的太跳跃了，前后衔接很差。突然提出这个概念，不知所云，使人抓狂。  \n",
    "状态访问分布$\\nu^{\\pi}(s) = (1-\\gamma)\\sum_{t=0}^{\\infty} \\gamma^t P^{\\pi}_t(s)$  \n",
    "首先，状态访问分布和占用度量，都是人为定义出来的值、构造出来的公式，类似状态价值和行动价值，是为了某种目的，而构造出来的。和稳定分布不太一样，稳定分布是对实际概率的推演，即固定策略和状态转移概率，长期运行后状态访问会稳定在一个分布上。   \n",
    "为什么要有状态访问分布这个定义呢？  \n",
    "因为，调整策略，其实就是在调整状态访问分布；而一个状态的状态价值，也和该状态访问之后的状态访问分布有关，为什么，因为不同状态的状态价值不同嘛，当前状态的状态价值又是未来访问状态价值的期望，期望不止有值，还有分布，可不分布影响了当前状态价值的值。  \n",
    "或者说，状态价值定义在期望之上，而期望由两部分组成，概率和值。而值其实又和未来的概率和值有关，是收敛得来的。所以最终，状态价值其实取决于概率和奖励，奖励才是真正输入进这个系统的信号。而策略，调整的是访问各状态获取奖励的分布。状态价值，是在这两者作用之下得到的收敛的值。  \n",
    "基于以上罗里吧嗦的一堆，可以认识到，状态访问概率，是挺重要的一个指标。所以，为了衡量这个指标，发明了以上公式，来描述长期的状态访问概率。它不是一个真实存在的值，再说一遍，但是它能衡量出长期的状态访问分布。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e490dc81-be50-40b9-ad42-49d2efcbdd44",
   "metadata": {},
   "source": [
    "#### 状态访问分布对应的贝尔曼方程\n",
    "$\\nu^{\\pi}(s') = (1-\\gamma)v_0(s') + \\gamma \\sum_{s\\in\\mathcal{S}}\\sum_{a\\in\\mathcal{A}}\\pi(a|s)p(s'|s,a) \\nu^{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced3744f-602d-439f-b458-35aead2b93ac",
   "metadata": {},
   "source": [
    "推导  \n",
    "$\\nu^{\\pi}(s') = (1-\\gamma)\\sum_{t=0}^{\\infty} \\gamma^t P^{\\pi}_t(s')$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba6f3d3-100f-4d4c-a7db-d35318c0f1e6",
   "metadata": {},
   "source": [
    "$\\nu^{\\pi}(s') = (1-\\gamma) (\\gamma ^0P_0^{\\pi}(s') + \\sum_{t=1}^{\\infty} \\gamma^t P_t^{\\pi}(s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514f22b2-54e7-4740-bab5-c4522ec60f52",
   "metadata": {},
   "source": [
    "$\\nu^{\\pi}(s') = (1-\\gamma)\\nu_0(s') + (1-\\gamma)\\sum_{t=1}^{\\infty} \\gamma^t P_t^{\\pi}(s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ef0c4c-04bc-4689-8dde-1d371f91ae71",
   "metadata": {},
   "source": [
    "这么一行一步比较舒服，能看到上一行，不然全在一个cell里不太好看前面写过的公式。  \n",
    "关注后半部分，按状态和动作展开："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54fc638-d7ca-4587-95bf-72303ee380d4",
   "metadata": {},
   "source": [
    "$(1-\\gamma)\\sum_{t=1}^{\\infty} \\gamma^t P_t^{\\pi}(S_t=s') = (1-\\gamma)\\sum_{t=1}^{\\infty} \\sum_{s\\in\\mathcal{S}} \\sum_{a\\in\\mathcal{A}} \\gamma^t \\pi(a|s)p(s'|s,a)P(S_{t-1}=s'|\\pi)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529aeac-bdb0-4630-bf51-d591e15af835",
   "metadata": {},
   "source": [
    "$= (1-\\gamma)\\sum_{s\\in\\mathcal{S}} \\sum_{a\\in\\mathcal{A}} \\pi(a|s)p(s'|s,a)\\sum_{t=1}^{\\infty} \\gamma^t P(S_{t-1}=s'|\\pi)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9182f-88ea-47d6-8699-5e896e5c9407",
   "metadata": {},
   "source": [
    "令$t'=t-1$, 则$t=t'+1$,$S_{t'}=S_{t-1}=s$代入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d56405-6c29-4b5e-a97a-f1bde28f056f",
   "metadata": {},
   "source": [
    "$= (1-\\gamma)\\sum_{s\\in\\mathcal{S}} \\sum_{a\\in\\mathcal{A}} \\pi(a|s)p(s'|s,a)\\sum_{t'=0}^{\\infty} \\gamma^{t'+1} P(S_{t'}=s|\\pi)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa4bb08-079e-4867-aeb2-ec6ff20d3f36",
   "metadata": {},
   "source": [
    "$= (1-\\gamma)\\sum_{s\\in\\mathcal{S}} \\sum_{a\\in\\mathcal{A}} \\pi(a|s)p(s'|s,a)\\gamma \\sum_{t'=0}^{\\infty} \\gamma^{t'} P(S_{t'}=s|\\pi)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3966cf0c-8812-4044-ae4b-f8c0e42ecf5e",
   "metadata": {},
   "source": [
    "按照定义：  \n",
    "$\\nu^{\\pi}(s) = (1-\\gamma)\\sum_{t=0}^{\\infty} \\gamma^t P^{\\pi}_t(s)$   \n",
    "$\\frac{\\nu^{\\pi}(s)}{(1-\\gamma)} = \\sum_{t=0}^{\\infty} \\gamma^t P^{\\pi}_t(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5278d625-71ea-4697-a571-d3ca88c15854",
   "metadata": {},
   "source": [
    "$= (1-\\gamma)\\sum_{s\\in\\mathcal{S}} \\sum_{a\\in\\mathcal{A}} \\pi(a|s)p(s'|s,a)\\gamma \\frac{1}{1-\\gamma} \\nu^{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2a96ae-7a56-458d-9a11-1f34329f1700",
   "metadata": {},
   "source": [
    "$= \\gamma\\sum_{s\\in\\mathcal{S}} \\sum_{a\\in\\mathcal{A}} \\pi(a|s)p(s'|s,a) \\nu^{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baf47d8-68a4-4223-94d2-4f5fa472232a",
   "metadata": {},
   "source": [
    "和前半部分合并代入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c943e84e-5b0b-4041-8aaf-b3b39675aed1",
   "metadata": {},
   "source": [
    "$\\nu^{\\pi}(s') = (1-\\gamma)\\nu_0(s') + \\gamma\\sum_{s\\in\\mathcal{S}} \\sum_{a\\in\\mathcal{A}} \\pi(a|s)p(s'|s,a) \\nu^{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b683f58-65d2-42f0-b4ca-0fd5353fb5f0",
   "metadata": {},
   "source": [
    "得证"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe85f3e9-81a8-47a6-99e7-ea78d37c850f",
   "metadata": {},
   "source": [
    "然后是所谓的占用度量。如果状态访问分布对应状态价值，占用度量就对应行动价值。  \n",
    "$\\rho_{\\pi}(s,a)=(1-\\gamma)\\sum_{t=0}^{\\infty} \\gamma^t P_t^{\\pi}(s)\\pi(a|s)$  \n",
    "$= (1-\\gamma)\\nu_{\\pi}(s) \\pi(a|s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7654c-ef38-460e-9671-b3070ec24e02",
   "metadata": {},
   "source": [
    "以上说来说去，其实是在说：策略影响了状态的访问分布。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
